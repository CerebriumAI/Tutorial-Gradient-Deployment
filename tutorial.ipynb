{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amnSfQ4S-im9"
      },
      "source": [
        "# Model Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO8e2gvThmSk"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP1me7fScNf-"
      },
      "source": [
        "Model deployment has always been an overly complex part of the MLOPs process. There is a lot of content online on how to build a model but most tutorials stop there. For content that does cover deployment, you have to spin up Kubernetes clustes and a load balencer to test sometimes the most basic of models. It is definitely getting easier to deploy models but we still think it has some way to go as deployment is just a small part of the puzzle.\n",
        "\n",
        "When we think about deploying a model there are a few questions we like to ask:\n",
        "* How quickly do I want to get this model up and running?\n",
        "* Is the model going into production or is it in a testing/beta phase?\n",
        "* How will I be serving data to this model? Streaming, batch or API inference?\n",
        "* What scale do we expect this model to reach?\n",
        "\n",
        "There are many other questions you should asked based on if you have existing architecture but in this tutorial we will be showing you how to use [Gradient](https://gradient.run/) to deploy your ML models. Gradient is a low-code Model deployment platform that abstracts alot of the complexities away from you such setting up infrastructure, scaling, frameworks, etc. To answer our own questions, this is when we like to use gradient:\n",
        "\n",
        "* Quickly (10 minutes)\n",
        "* Production or just quickly testing with a subset of users.\n",
        "* API inference\n",
        "* We would be comfortale with Gradient handling a few 1000's transactions per second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlGloFVZhvm8"
      },
      "source": [
        "### What are you going to learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64JkIOigh1Ul"
      },
      "source": [
        "In this tutorial, we will be building a transaction fraud detection model, and deploying it as a REST API using Gradient.\n",
        "\n",
        "By the end of this tutorial you will be able to:\n",
        "- Add a dataset in Gradient\n",
        "- Create a FastAPI application for your ML Service\n",
        "- Deploy your machine learning service to Gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbcGiuf0lXJe"
      },
      "source": [
        "## Tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmmF930ilbI9"
      },
      "source": [
        "Prerequisites:\n",
        "- Install Python3.8+\n",
        "- Install JupyterLab\n",
        "\n",
        "You should download the data required for this tutorial from [here](https://drive.google.com/file/d/1MidRYkLdAV-i0qytvsflIcKitK4atiAd/view?usp=sharing). This is originally from a [Kaggle dataset](https://www.kaggle.com/competitions/ieee-fraud-detection/data) for Fraud Detection. Place this dataset in a `data` directory in the root of your project. You can run this notebook either in VS Code, Jupyter Notebooks or Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC91cCyFcNgA"
      },
      "source": [
        "### Build a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEydtTZycNgA"
      },
      "source": [
        "Firstly, let's build a quick model to detect fraudulent transactions. Model building is out of scope for this tutorial but we build an extremely basic model.\n",
        "\n",
        "We will need a number of libraries so lets install them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2sV8mICcNgA",
        "outputId": "4a0ae89d-44f5-4c65-8516-d9abf1029105",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn==1.0.2 pandas==1.4.3 numpy==1.23.2 xgboost==1.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWI28p60-imy",
        "outputId": "f9ec5f7e-9a12-44fd-d6a7-d354847b7cd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['xgb_fraud_model.joblib']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the data, sample such that the target classes are equal size\n",
        "df = pd.read_csv(\"train_transaction.csv\")\n",
        "df = pd.concat([df[df.isFraud == 0].sample(n=len(df[df.isFraud == 1])), df[df.isFraud == 1]], axis=0)\n",
        "\n",
        "# Select the features and target\n",
        "X = df[[\"ProductCD\", \"P_emaildomain\", \"R_emaildomain\", \"card4\", \"M1\", \"M2\", \"M3\"]]\n",
        "y = df.isFraud\n",
        "\n",
        "# Use one-hot encoding to encode the categorical features\n",
        "enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "enc.fit(X)\n",
        "\n",
        "X = pd.DataFrame(enc.transform(X).toarray(), columns=enc.get_feature_names_out().reshape(-1))\n",
        "X[\"TransactionAmt\"] = df[[\"TransactionAmt\"]].to_numpy()\n",
        "\n",
        "# Split the dataset and train the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
        "model = xgb.fit(X_train, y_train)\n",
        "\n",
        "# Save the model and encoder\n",
        "import joblib\n",
        "joblib.dump(enc, \"ohe_fraud_encoder.joblib\")\n",
        "joblib.dump(model, \"xgb_fraud_model.joblib\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH8jh3DacNgD"
      },
      "source": [
        "When you're done, create a requirements.txt file in the root of your project and fill it with the following lines. These are the versions, we ran our models with, if yours differ please update them.\n",
        "\n",
        "```\n",
        "xgboost==1.5.1\n",
        "pandas\n",
        "numpy\n",
        "fastapi\n",
        "uvicorn\n",
        "joblib\n",
        "scikit-learn==1.0.2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcLh-wOfcNgD"
      },
      "source": [
        "### Create a FastAPI Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp7FDgamcNgD"
      },
      "source": [
        "Now, we need to create an application that will serve our model. We are going to use the [FastAPI](https://fastapi.tiangolo.com/) framework, which is a lightweight, fast, and flexible framework for building REST APIs in Python.\n",
        "\n",
        "Save the following in a file called `app.py` in the root of your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rts4WzzccNgE"
      },
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "import joblib\n",
        "\n",
        "from fastapi import FastAPI, Request\n",
        "import uvicorn\n",
        "\n",
        "ENCODER_PATH = \"models/ohe_fraud_encoder.joblib\"\n",
        "MODEL_PATH = \"models/xgb_fraud_model.joblib\"\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def health_check():\n",
        "    return \"Healthy!\"\n",
        "\n",
        "\n",
        "@app.post(\"/fraud-classfier\")\n",
        "async def fraud_prediction(request: Request):\n",
        "    request_data = await request.json()\n",
        "    df = pd.DataFrame(request_data)\n",
        "\n",
        "    # Preprocessing\n",
        "    categorical_cols = [\n",
        "        \"ProductCD\",\n",
        "        \"P_emaildomain\",\n",
        "        \"R_emaildomain\",\n",
        "        \"card4\",\n",
        "        \"M1\",\n",
        "        \"M2\",\n",
        "        \"M3\",\n",
        "    ]\n",
        "    X = df[categorical_cols]\n",
        "    enc = joblib.load(ENCODER_PATH)\n",
        "    X = pd.DataFrame(\n",
        "        enc.transform(X).toarray(), columns=enc.get_feature_names_out().reshape(-1)\n",
        "    )\n",
        "    X[\"TransactionAmt\"] = df[[\"TransactionAmt\"]].to_numpy()\n",
        "\n",
        "    # XGBoost Classifier\n",
        "    model = joblib.load(MODEL_PATH)\n",
        "    pred = model.predict(X)\n",
        "\n",
        "    response_map = {0: \"Legitimate\", 1: \"Fraud\"}\n",
        "    return [response_map[prediction] for prediction in pred]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T_D7QMsmvrc"
      },
      "source": [
        "As you can see our code hasn't changed much from our original model. What we have done is:\n",
        "\n",
        "* Imported the encoder and model file that we trained\n",
        "* Create a API endpoint for / to check the health status of our API as well as /fraud-classifer which will do all the work to return to the user their prediction\n",
        "* Transform the variables from the API request and run the model predict function and return if the transaction was fraudulent or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_YukxjHcNgE"
      },
      "source": [
        "You can then run the API with the command:\n",
        "```python\n",
        "python app.py\n",
        "```\n",
        "Test the application by going to http://localhost:8000/fraud-classfier, and sending a request with the following data:\n",
        "\n",
        "```json\n",
        "[{\n",
        "    \"TransactionID\":3366167,\n",
        "    \"isFraud\":0,\n",
        "    \"TransactionAmt\":495.0,\n",
        "    \"ProductCD\":\"W\",\n",
        "    \"card4\":\"visa\",\n",
        "    \"P_emaildomain\":\"live.com\",\n",
        "    \"R_emaildomain\":null,\n",
        "    \"M1\":\"T\",\n",
        "    \"M2\":\"T\",\n",
        "    \"M3\":\"T\"\n",
        "}]\n",
        "```\n",
        "\n",
        "The output should be `[1]` indicating that the transaction is fraudulent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pig9pZbPcNgF"
      },
      "source": [
        "### Deployment A: Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSd79-uzcNgF"
      },
      "source": [
        "Great! Now we have a model and an application. We need to deploy it.\n",
        "\n",
        "Navigate to https://gradient.run/ and hit the *Sign up* button.\n",
        "\n",
        "![Gradient Landing Page](media/grad_landing.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJnTm36cNgF"
      },
      "source": [
        "Once you have signed up, you should now be greeted with a page that prompts you to create a new project. However, in order to use Gradient, you will need to enroll in the **Pro Plan**, so let's do that first. Hit the upgrade button at the top of the page and follow the prompts to upgrade.\n",
        "\n",
        "![Gradient Create Project](media/grad_new_project.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pKBEumjcNgG"
      },
      "source": [
        "Once you're done, hit the *Back To Console* button to head back to the create a project screen, and then hit the *Create Project* button. Name the project *fraud-classifier*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_UnPNR0cNgG"
      },
      "source": [
        "In order to deploy, we need to create a **Gradient Dataset**. Gradient will use this as local storage during application building. Navigate to the *Data* tab and hit the *Create A Dataset* button. Name the dataset *fraud-dataset*. \n",
        "\n",
        "![Gradient Create Dataset](media/grad_data.png)\n",
        "\n",
        "You can then hit the back button to return to the *Data* tab and you should see the dataset.\n",
        "\n",
        "![Gradient Dataset](media/grad_datasets.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr_7gzMqcNgG"
      },
      "source": [
        "Time to create a deployment! Navigate to the *Deployment* tab and hit the *Create Deployment* button. You should be greeted with a screen that looks like this:\n",
        "\n",
        "![Gradient Create Deployment](media/grad_deployment.png)\n",
        "\n",
        "Have your console at the ready! We are going to use the terminal to deploy our application. First thing to do is install gradient.\n",
        "\n",
        "```bash\n",
        "pip install -U gradient\n",
        "```\n",
        "\n",
        "Side note: I was getting a TypeError running any Gradient command. I use Conda as my package dependancy so instead of doing pip install -U gradient I did conda install gradient and then everything worked after that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKlr5hs-cNgG"
      },
      "source": [
        "Once the package is installed, we need to authenticate. We will use the *gradient* CLI to do this. Use the `apiKey` command to authenticate, pasting in your API key as the argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6lGL2fScNgH"
      },
      "outputs": [],
      "source": [
        "# !gradient apiKey <your-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRjsaIrYcNgH"
      },
      "source": [
        "To continue, you will need to push your code to a Github repository. You can either create a new **public** repository and push your code to the master branch Note the url of the repository. You will need the following files as part of your repo:\n",
        "- app.py\n",
        "- requirements.txt\n",
        "- xgb_fraud_model.joblib\n",
        "- ohe_fraud_encoder.joblib\n",
        "\n",
        "Instead of creating a new repository, you can also fork our repository and edit your deployment.yaml as specified below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyJa4hzrcNgH"
      },
      "source": [
        "As I mentioned previously, Gradient uses a simple deployment file to deploy your application.\n",
        "In the root of your project, create a file called `deployment.yaml` and save the following contents (fill out the id of the `fraud-dataset` and the URL of your Github repository). If you are forking the repository, just update the values in the deployment.yaml\n",
        "\n",
        "```yaml\n",
        "\n",
        "```yaml\n",
        "image: python:3.8-bullseye\n",
        "port: 8000\n",
        "command:\n",
        "  - /bin/sh\n",
        "  - '-c'\n",
        "  - |\n",
        "    cd /opt/repos/repo\n",
        "    pip install -r requirements.txt\n",
        "    python app.py\n",
        "repositories:\n",
        "  dataset: <fraud-dataset-id>\n",
        "  mountPath: /opt/repos\n",
        "  repositories:\n",
        "    - url: <your-github-repo-url>\n",
        "      name: repo\n",
        "      ref: master\n",
        "resources:\n",
        "  replicas: 1\n",
        "  instanceType: C5\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzkP3UW9cNgI"
      },
      "source": [
        "Key concepts to mention in this deployment file:\n",
        "- `image`: The image to use for the container.\n",
        "- `port`: The port to expose the application on.\n",
        "- `command`: The series of commands to run in the container. In this case, we install our neeeded packages and then run the application.\n",
        "- `repositories`: The repositories to mount in the container. We specify the dataset to use for local storage, the mount path and the Github repository to pull the code from. The dataset stays up to date with the git repository.\n",
        "- `resources`: The resources to allocate to the container. We specify the number of replicas and the instance type (use the C5 instance to avoid any hiccups). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PehduwLGcNgI"
      },
      "source": [
        "Finally, we can deploy. Use the `deployments` command to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03ejEJqScNgI"
      },
      "outputs": [],
      "source": [
        "# !gradient deployments create --name <your-service-name> --projectId <your-project-id> --spec ./deployment.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTa1hJrtcNgI"
      },
      "source": [
        "We're done! You can watch the service build in the *Workflows* tab. \n",
        "\n",
        "![Gradient Workflows](media/grad_workflow.png)\n",
        "\n",
        "\n",
        "Once it's built, navigate to the *Deployments* tab and select your service. Click the endpoint URL to see the application running (it should say *Healthy!*). Give it about 5 minutes once it has finished building before you check if its working.\n",
        "\n",
        "![Gradient Final Deployment](media/grad_endpoint.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z020jjquSHn_"
      },
      "source": [
        "You can now send a POST request to endpoint as before to ensure inference is running correctly! Sent the following JSON Post request to `<YOUR_ENDPOINT>/fraud-classfier`\n",
        "\n",
        "```\n",
        "{\n",
        "  \"ProductCD\": \"H\",\n",
        "  \"P_emaildomain\": \"gmail.com\",\n",
        "  \"R_emaildomain\": \"\",\n",
        "  \"card4\": \"visa\",\n",
        "  \"M1\": \"\",\n",
        "  \"M2\": \"\",\n",
        "  \"M3\": \"\"\n",
        "}\n",
        "```\n",
        "\n",
        "The output should be: **Legitimate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xcBtf3B04h8"
      },
      "source": [
        "## Conslusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FcePJSg1E3U"
      },
      "source": [
        "That is it for our tutorial on how to deploy your ML model to production quickly! This is just one method of deployment among a sea of other however this should work well for most API based use-cases."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "aO8e2gvThmSk",
        "KlGloFVZhvm8",
        "BC91cCyFcNgA",
        "EcLh-wOfcNgD"
      ],
      "name": "tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
